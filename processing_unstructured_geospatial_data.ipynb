{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "Intro",
    "collapsed": false,
    "resultHeight": 442
   },
   "source": "## Intro\nIn this quickstart guide, we will show you how to read geospatial data from unstructured sources such as GeoTiffs and Shapefiles to prepare features for a machine learning model using Snowflake and popular Python geospatial libraries.\n\nYou will learn how to join data from different sources to help predict the presence of groundwater. Although the prediction step itself is out of scope for this lab, you will learn how to ingest data from raster files and shapefiles and combine them using nearst neighbour approach.\n\nIn this lab, we will use the following sources:\n- Elevation map from [United States Geological Survey](https://www.usgs.gov/).\n- Average precipitation and average temperature data from [WorldClim](https://www.worldclim.org/data/worldclim21.html).\n\nThe result of this lab will be a single dataset containing information derived from the above sources.\n\nSince we will be running relatively complex computations using Snowpark, you will need a `LARGE` Snowpark-optimized warehouse. Run the following query to create one:"
  },
  {
   "cell_type": "code",
   "id": "797b25e0-9289-4017-a011-88aa834fa07b",
   "metadata": {
    "language": "sql",
    "name": "sql_1",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE WAREHOUSE IF NOT EXISTS snowpark_opt_wh_l WITH warehouse_size = 'LARGE' warehouse_type = 'SNOWPARK-OPTIMIZED';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "name": "Step_1_Data_Acquisition",
    "collapsed": false,
    "resultHeight": 170
   },
   "source": "Now you can go to notebook settings and set the newly created warehouse as the `SQL warehouse`. Additionally, go to the Packages dropdown and import `branca`, `pydeck` and `rasterio`, which you will use in this lab.\n\n## Step 1. Data Acquisition\nAs a first step, you will attach an external stage with raster and shapefiles. Run the following queries:"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "sql_2",
    "collapsed": false,
    "resultHeight": 112
   },
   "source": "CREATE DATABASE IF NOT EXISTS ADVANCED_ANALYTICS;\nCREATE SCHEMA IF NOT EXISTS ADVANCED_ANALYTICS.RASTER;\nUSE SCHEMA ADVANCED_ANALYTICS.RASTER;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d19342a2-93af-44fb-8173-9145ecf322e5",
   "metadata": {
    "language": "sql",
    "name": "sql_3",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE STAGE ADVANCED_ANALYTICS.RASTER.FILES URL = 's3://sfquickstarts/hol_geo_spatial_ml_using_snowflake_cortex/unstructured/';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50e3b5c2-7ab6-4608-81f4-fcc63ddec5b0",
   "metadata": {
    "name": "Step_2_Loading_Raster",
    "collapsed": false,
    "resultHeight": 448
   },
   "source": "For this lab, you will also use a native application called [SedonaSnow](https://app.snowflake.com/marketplace/listing/GZTYZF0RTY3/wherobots-sedonasnow), which contains more than a hundred geospatial functions.\n\n* Navigate to the `Marketplace` screen using the menu on the left side of the window.\n* Search for `SedonaSnow` in the search bar.\n* Once in the listing, click the big blue `Get` button.\n\nOn the `Get` screen, you may be prompted to complete your user profile if you have not done so before. Click the link as shown in the screenshot below. Enter your name and email address into the profile screen and click the blue Save button. You will be returned to the `Get` screen.\n\nCongratulations, you have now acquired all the data sources that you need for this lab.\n\n## Step 2. Loading Raster Data\nIn this step, you will load data from raster files stored in an external stage and store it as a Snowflake table.\n\nYou will start with elevation data. Let's first create a function that uses the Python library `rasterio`, available in the [Snowflake Conda Channel](https://repo.anaconda.com/pkgs/snowflake/), which reads metadata from a `GeoTiff` file stored in a stage. Run the following query:"
  },
  {
   "cell_type": "code",
   "id": "c125e359-9484-4f49-a425-0109abe17742",
   "metadata": {
    "language": "sql",
    "name": "sql_4",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION ADVANCED_ANALYTICS.RASTER.PY_EXTRACT_GEOTIFF_METADATA(PATH_TO_FILE STRING)\nRETURNS TABLE (\n    status BOOLEAN,\n    error STRING,\n    band_count INT,\n    crs STRING,\n    bounds STRING,\n    metadata STRING\n)\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('rasterio', 'snowflake-snowpark-python')\nHANDLER = 'GeoTiffMetadataExtractor'\nAS $$\nimport rasterio\nimport json\nfrom snowflake.snowpark.files import SnowflakeFile\n\nclass GeoTiffMetadataExtractor:\n    def process(self, PATH_TO_FILE: str):\n        try:\n            # Initialize the result variables\n            status = False\n            error = ''\n            band_count = None\n            crs = None\n            bounds_json = None\n            metadata_json = None\n\n            # Read the GeoTIFF file from the specified stage path into memory\n            with SnowflakeFile.open(PATH_TO_FILE, 'rb', require_scoped_url=False) as input_file:\n                tif_bytes = input_file.read()\n\n            # Use rasterio's MemoryFile to read the TIFF data from memory\n            with rasterio.MemoryFile(tif_bytes) as memfile:\n                with memfile.open() as dataset:\n                    # Extract metadata from the dataset\n                    band_count = dataset.count\n                    crs = str(dataset.crs)  # Convert CRS to string for serialization\n                    bounds = dataset.bounds._asdict()  # Convert bounds to a dictionary\n\n                    # Ensure that metadata is serializable\n                    metadata = dataset.meta.copy()\n                    # Convert 'transform' to a tuple\n                    if 'transform' in metadata:\n                        metadata['transform'] = metadata['transform'].to_gdal()\n                    # Convert 'crs' to string\n                    if 'crs' in metadata:\n                        metadata['crs'] = str(metadata['crs'])\n\n                    # Convert bounds and metadata to JSON strings\n                    bounds_json = json.dumps(bounds)\n                    metadata_json = json.dumps(metadata)\n\n                    # Parsing successful\n                    status = True\n\n        except Exception as e:\n            # Handle exceptions, such as corrupted files\n            error = str(e)\n            status = False\n\n        # Yield the result as a single row\n        yield (\n            status,\n            error,\n            band_count,\n            crs,\n            bounds_json,\n            metadata_json\n        )\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "feade95e-0838-42e0-b11e-ce9b2c029c5c",
   "metadata": {
    "name": "markdown_1",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Additionally, you will create a function to check the distribution of bands. Sometimes, bands of certain values prevail over others, and stripping them off during loading of data from raster files can significantly reduce the size of the table."
  },
  {
   "cell_type": "code",
   "id": "aac31d3d-0ccb-413f-b678-4a631a16001b",
   "metadata": {
    "language": "sql",
    "name": "sql_5",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION ADVANCED_ANALYTICS.RASTER.PY_RASTER_BAND_VALUE_STATS(PATH_TO_FILE STRING)\nRETURNS TABLE (\n    band_value FLOAT,\n    count BIGINT,\n    percentage FLOAT\n)\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('numpy', 'rasterio', 'snowflake-snowpark-python')\nHANDLER = 'RasterBandValueStats'\nAS $$\nimport numpy as np\nimport rasterio\nfrom snowflake.snowpark.files import SnowflakeFile\n\nclass RasterBandValueStats:\n    def process(self, PATH_TO_FILE: str):\n        try:\n            # Read the GeoTIFF file from the specified stage path\n            with SnowflakeFile.open(PATH_TO_FILE, 'rb', require_scoped_url=False) as input_file:\n                tif_bytes = input_file.read()  # Read the entire file into bytes\n\n            # Use rasterio's MemoryFile to read the TIFF data from memory\n            with rasterio.MemoryFile(tif_bytes) as memfile:\n                with memfile.open() as dataset:\n                    # Read all bands into a NumPy array\n                    data = dataset.read()  # Shape: (band_count, rows, cols)\n\n                    # Flatten the data across all bands\n                    data_flat = data.flatten()  # 1D array of all pixel values across all bands\n\n                    # Count unique values\n                    unique_values, counts = np.unique(data_flat, return_counts=True)\n\n                    # Calculate total number of values\n                    total_count = data_flat.size\n\n                    # Calculate percentage for each unique value\n                    percentages = (counts / total_count) * 100\n\n                    # Yield results\n                    for value, count, percentage in zip(unique_values, counts, percentages):\n                        yield (\n                            float(value),\n                            int(count),\n                            round(float(percentage),1)\n                        )\n        except Exception as e:\n            raise Exception(f\"Error during data extraction: {e}\")\n$$;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e7c04f14-4245-4e0d-84b2-c32f97de914d",
   "metadata": {
    "name": "markdown_2",
    "collapsed": false,
    "resultHeight": 160
   },
   "source": "Next, you will create a function that reads data from a `GeoTIFF` file and outputs it as a table. The UDF you will create processes each pixel in the `GeoTIFF` file by calculating the spatial coordinates (X and Y) of the centroid of the pixel. It then associates these coordinates with the pixel's corresponding band values.\n\nThis approach transforms the raster image into a collection of spatial points enriched with attribute data (band values), making it suitable for vector-based analyses and database operations. Run the following query:"
  },
  {
   "cell_type": "code",
   "id": "908a03cc-57c5-4d1c-bb9c-b58f58635e82",
   "metadata": {
    "language": "sql",
    "name": "sql_6",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION ADVANCED_ANALYTICS.RASTER.PY_LOAD_GEOTIFF(\n    PATH_TO_FILE STRING,\n    SKIP_VALUES ARRAY DEFAULT NULL  -- Make SKIP_VALUES optional with default NULL\n)\nRETURNS TABLE (\n    x FLOAT,\n    y FLOAT,\n    band_values ARRAY,\n    band_count INT\n)\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('numpy', 'rasterio', 'snowflake-snowpark-python')\nHANDLER = 'GeoTiffExtractor'\nAS $$\nimport numpy as np\nimport rasterio\nfrom snowflake.snowpark.files import SnowflakeFile\n\nclass GeoTiffExtractor:\n    def process(self, PATH_TO_FILE: str, SKIP_VALUES=None):\n        try:\n            # Read the GeoTIFF file from the specified stage path\n            with SnowflakeFile.open(PATH_TO_FILE, 'rb', require_scoped_url=False) as input_file:\n                tif_bytes = input_file.read()  # Read the entire file into bytes\n\n            # Use rasterio's MemoryFile to read the TIFF data from memory\n            with rasterio.MemoryFile(tif_bytes) as memfile:\n                with memfile.open() as dataset:\n                    # Read all bands into a NumPy array\n                    data = dataset.read()  # Shape: (band_count, rows, cols)\n\n                    # Get the number of bands\n                    band_count = data.shape[0]\n\n                    # Get the coordinates\n                    rows, cols = np.indices((dataset.height, dataset.width))\n                    xs, ys = rasterio.transform.xy(\n                        dataset.transform, rows, cols, offset='center'\n                    )\n\n                    # Flatten the arrays\n                    xs = np.array(xs).flatten()\n                    ys = np.array(ys).flatten()\n                    pixel_values = data.reshape((band_count, -1)).T  # Shape: (num_pixels, band_count)\n\n                    # Handle SKIP_VALUES\n                    if SKIP_VALUES:\n                        # Convert SKIP_VALUES to a NumPy array for efficient comparison\n                        skip_values = np.array(SKIP_VALUES)\n\n                        # Create a mask for pixels to skip\n                        skip_mask = np.isin(pixel_values, skip_values).any(axis=1)\n                        # Invert the skip_mask to get the mask of pixels to keep\n                        mask = ~skip_mask\n\n                        # Apply the mask to xs, ys, and pixel_values\n                        xs_filtered = xs[mask]\n                        ys_filtered = ys[mask]\n                        pixel_values_filtered = pixel_values[mask]\n                    else:\n                        # If SKIP_VALUES not provided, use all data\n                        xs_filtered = xs\n                        ys_filtered = ys\n                        pixel_values_filtered = pixel_values\n\n                    # For each pixel, yield a row with x, y, and band values\n                    for i in range(len(xs_filtered)):\n                        # Get the pixel values for all bands\n                        band_vals = pixel_values_filtered[i].tolist()\n                        yield (\n                            xs_filtered[i],\n                            ys_filtered[i],\n                            band_vals,\n                            band_count\n                        )\n        except Exception as e:\n            raise Exception(f\"Error during data extraction: {e}\")\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50e2a61b-1a57-4e26-b7bf-bdc1d6beeee8",
   "metadata": {
    "name": "markdown_3",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Now you will check the metadata of the elevation file. Run the following query:"
  },
  {
   "cell_type": "code",
   "id": "90b8c112-8a85-4ac6-a9f9-efc5174ce7b4",
   "metadata": {
    "language": "sql",
    "name": "sql_7",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "SELECT *\nFROM table(ADVANCED_ANALYTICS.RASTER.PY_EXTRACT_GEOTIFF_METADATA(build_scoped_file_url(@ADVANCED_ANALYTICS.RASTER.FILES,'ASTGTMV003_N07E033_dem.tif')));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9faa0839-5454-400d-b4db-b85dfb5b3447",
   "metadata": {
    "name": "markdown_4",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "As you can see, the GeoTiff uses reference system `EPSG:4326` which means you can store it as `GEOGRAPHY` type. Run the following query to check how bands are distributed inside of the raster."
  },
  {
   "cell_type": "code",
   "id": "f0fbf931-2b47-4e05-9ad6-021ce587ffb9",
   "metadata": {
    "language": "sql",
    "name": "sql_8",
    "collapsed": false,
    "resultHeight": 439
   },
   "outputs": [],
   "source": "\nSELECT *\nFROM table(ADVANCED_ANALYTICS.RASTER.PY_RASTER_BAND_VALUE_STATS(build_scoped_file_url(@ADVANCED_ANALYTICS.RASTER.FILES, 'ASTGTMV003_N07E033_dem.tif')));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "080c6eea-e00e-4c0c-b630-6453c8673b55",
   "metadata": {
    "name": "markdown_5",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "There are no obvious outliers among band values—those that correspond to most of the raster points. In this case, let's load data from the whole `ASTGTMV003_N07E033_dem.tif` into the table `POC.RASTER.AFRICA_ELEVATION`:"
  },
  {
   "cell_type": "code",
   "id": "1b19de3e-80f2-44f4-84e4-a10ce070e57a",
   "metadata": {
    "language": "sql",
    "name": "sql_9",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE ADVANCED_ANALYTICS.RASTER.AFRICA_ELEVATION AS\nSELECT st_makepoint(x, y) as geog,\nband_values[0]::float as band\nFROM table(ADVANCED_ANALYTICS.RASTER.PY_LOAD_GEOTIFF(build_scoped_file_url(@ADVANCED_ANALYTICS.RASTER.FILES, 'ASTGTMV003_N07E033_dem.tif')));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "006760b7-99de-44c5-8fa1-6b6ffe50a9e4",
   "metadata": {
    "name": "markdown_6",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Let's check the size of the newly created table. Run the following query:"
  },
  {
   "cell_type": "code",
   "id": "e8dc3b1a-8d94-4a99-bdf5-ab2a3b507996",
   "metadata": {
    "language": "sql",
    "name": "sql_10",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "SELECT count(*) FROM ADVANCED_ANALYTICS.RASTER.AFRICA_ELEVATION",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4523de31-2150-4d65-b51a-401ac13cfc19",
   "metadata": {
    "name": "markdown_7",
    "collapsed": false,
    "resultHeight": 310
   },
   "source": "The number of rows is a product of width and height in pixels. In the case of the elevation file, it's 3601×3601. Some raster files might be quite large, and to process them, it might be a good idea to use Snowpark-optimized warehouses to avoid memory exhaustion issues. Another technique that you could apply is to load not all points from the raster file but only those that contain useful information. Alternatively, you can resample large files to reduce their resolution. We will show you an example of how to do this at the end of this lab.\n\nBut 13M rows is also a rather large table, and visualizing its results using Python libraries might be challenging without reducing the number of rows. H3 functions can help with that. In the code below, you will do the following:\n\n- Map each point from `POC.RASTER.AFRICA_ELEVATION` to an H3 cell with resolution 8.\n- Group by H3 Cell ID and calculate the average value of the band for each H3 cell.\n- Visualize the H3 cells, using the band as the source for color coding."
  },
  {
   "cell_type": "code",
   "id": "eccd7b5e-e88f-41af-a27e-408e5536bbfe",
   "metadata": {
    "language": "python",
    "name": "python_1",
    "collapsed": false,
    "resultHeight": 523
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\nimport pydeck as pdk\nfrom typing import List\nimport branca.colormap as cm\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Execute the updated SQL query\ndf = session.sql('''select h3_point_to_cell_string(geog, 8) as h3_cell,\n                    st_x(h3_cell_to_point(h3_cell)) as lon,\n                    st_y(h3_cell_to_point(h3_cell)) as lat,\n                    avg(band) as band \n                    from ADVANCED_ANALYTICS.RASTER.AFRICA_ELEVATION\n                    group by all;''').to_pandas()\n\ndf[\"BAND\"] = df[\"BAND\"].apply(lambda row: float(row))\ncenter_latitude = df['LAT'].mean()\ncenter_longitude = df['LON'].mean()\n\ndef get_quantiles(df_column: pd.Series, quantiles: List) -> pd.Series:\n    return df_column.quantile(quantiles)\n\ndef get_color(df_column: pd.Series, colors: List, vmin: int, vmax: int, index: pd.Series) -> pd.Series:\n    color_map = cm.LinearColormap(colors, vmin=vmin, vmax=vmax, index=index)\n    return df_column.apply(color_map.rgb_bytes_tuple)\n    \nquantiles = get_quantiles(df[\"BAND\"], [0, 0.2, 0.4, 0.6, 0.8, 1])\ncolors = ['gray','blue','green','yellow','orange','red']\n\ndf['BAND'] = get_color(df['BAND'], colors, quantiles.min(), quantiles.max(), quantiles)\n\nst.pydeck_chart(pdk.Deck(\n    map_style=None,\n    initial_view_state=pdk.ViewState(\n        latitude=center_latitude,\n        longitude=center_longitude, \n        zoom=8.7, \n        bearing=0, \n        pitch=0),\n    layers=[\n        pdk.Layer(\n            \"H3HexagonLayer\",\n            df,\n            opacity=0.9,\n            stroked=False,\n            get_hexagon=\"H3_CELL\",\n            get_fill_color='BAND',\n            extruded=False,\n            wireframe=True,\n            line_width_min_pixels=0,\n            auto_highlight=True,\n            pickable=False,\n            filled=True\n        )\n    ],\n))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09700735-08fe-4b58-b24a-469a620aa6eb",
   "metadata": {
    "name": "Step_3_Load_Shapefile",
    "collapsed": false,
    "resultHeight": 128
   },
   "source": "## Step 3. Load Shapefile\n\nIn this step you will load precipitation and average temperature data from a Shapefile. First, you will create a UDF that uses [Dynamic file Access](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-examples#reading-a-dynamically-specified-file-with-snowflakefile) and `fiona` library to read metadata from Shapefile. Run the following code:"
  },
  {
   "cell_type": "code",
   "id": "fa7bf59b-5bf9-452b-bde8-e7586e27c5f2",
   "metadata": {
    "language": "sql",
    "name": "sql_12",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION ADVANCED_ANALYTICS.RASTER.PY_LOAD_GEOFILE_METADATA(PATH_TO_FILE string, filename string)\nRETURNS TABLE (metadata variant)\nLANGUAGE python\nRUNTIME_VERSION = 3.8\nPACKAGES = ('fiona', 'snowflake-snowpark-python')\nHANDLER = 'GeoFileReader'\nAS $$\n# Import necessary modules for file handling and geospatial data processing\nfrom snowflake.snowpark.files import SnowflakeFile\nfrom fiona.io import ZipMemoryFile\nimport fiona\n\n# Helper function to make objects JSON-serializable\ndef make_serializable(obj):\n    if isinstance(obj, dict):\n        # Recursively process dictionary items\n        return {k: make_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, (list, tuple)):\n        # Recursively process lists and tuples\n        return [make_serializable(v) for v in obj]\n    elif isinstance(obj, (int, float, str, bool, type(None))):\n        # Base case: object is already serializable\n        return obj\n    else:\n        # Convert non-serializable objects to strings\n        return str(obj)\n\n# Define the handler class for the UDF\nclass GeoFileReader:\n    def process(self, PATH_TO_FILE: str, filename: str):\n        # Enable support for KML drivers in Fiona\n        fiona.drvsupport.supported_drivers['libkml'] = 'rw'\n        fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n\n        # Open the file from the Snowflake stage in binary read mode\n        with SnowflakeFile.open(PATH_TO_FILE, 'rb') as f:\n            # Read the zip file into memory using Fiona's ZipMemoryFile\n            with ZipMemoryFile(f) as zip:\n                # Open the specified file within the zip archive\n                with zip.open(filename) as collection:\n                    # Extract metadata from the collection\n                    metadata = {\n                        'driver': collection.driver,  # File format driver (e.g., 'ESRI Shapefile')\n                        'crs': collection.crs.to_string() if collection.crs else None,  \n                        'schema': collection.schema,  # Schema of the data (fields and types)\n                        'bounds': collection.bounds,  # Spatial bounds of the dataset\n                        'meta': collection.meta,      # Additional metadata\n                        'name': collection.name,      # Name of the collection\n                        'encoding': collection.encoding,  # Character encoding of the file\n                        'length': len(collection),    # Number of features in the dataset\n                    }\n                    # Ensure the metadata is serializable to JSON\n                    serializable_metadata = make_serializable(metadata)\n                    # Yield the metadata as a tuple (required for UDFs returning TABLE)\n                    yield (serializable_metadata,)\n$$;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e15c1303-ef8d-43bf-8523-d1dc8dba34e8",
   "metadata": {
    "name": "markdown_8",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "The UDF above can be used not only to read metadata from Shapefiles but also from other types of geo files, such as KML. You will need another UDF for reading data from geo formats, including Shapefiles. To create one, run the following query:"
  },
  {
   "cell_type": "code",
   "id": "2362332b-d654-45f6-85d1-784631e49659",
   "metadata": {
    "language": "sql",
    "name": "sql_13",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION ADVANCED_ANALYTICS.RASTER.PY_LOAD_GEOFILE(PATH_TO_FILE string, filename string)\nRETURNS TABLE (wkt string, properties object)\nLANGUAGE python\nRUNTIME_VERSION = 3.8\nPACKAGES = ('fiona', 'shapely', 'snowflake-snowpark-python')\nHANDLER = 'GeoFileReader'\nAS $$\n# Import necessary modules for geometry handling and file operations\nfrom shapely.geometry import shape\nfrom snowflake.snowpark.files import SnowflakeFile\nfrom fiona.io import ZipMemoryFile\nimport fiona\n\n# Define the handler class for the UDF\nclass GeoFileReader:\n    def process(self, PATH_TO_FILE: str, filename: str):\n        # Enable support for KML drivers in Fiona\n        fiona.drvsupport.supported_drivers['libkml'] = 'rw'\n        fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n\n        # Open the file from the Snowflake stage in binary read mode\n        with SnowflakeFile.open(PATH_TO_FILE, 'rb') as f:\n            # Read the zip file into memory using Fiona's ZipMemoryFile\n            with ZipMemoryFile(f) as zip:\n                # Open the specified geospatial file within the zip archive\n                with zip.open(filename) as collection:\n                    # Iterate over each feature (record) in the collection\n                    for record in collection:\n                        # Check if the geometry is not None\n                        if record['geometry'] is not None:\n                            # Convert the geometry to Well-Known Text (WKT) format\n                            wkt = shape(record['geometry']).wkt\n                            # Convert the properties to a dictionary\n                            properties = dict(record['properties'])\n                            # Yield the WKT and properties as a tuple\n                            yield (wkt, properties)\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af39a647-360b-46a9-82f3-b5cf4c1f0b0d",
   "metadata": {
    "name": "markdown_9",
    "collapsed": false,
    "resultHeight": 42
   },
   "source": "Now you can look into the metadata of `WorldClim.shp` stored in `WorldClim.zip` package:"
  },
  {
   "cell_type": "code",
   "id": "293d2a8c-e15a-4c7c-8fca-d8f87a1e85ba",
   "metadata": {
    "language": "sql",
    "name": "sql_14",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "SELECT parse_json(metadata):crs as metadata\nFROM table(ADVANCED_ANALYTICS.RASTER.PY_LOAD_GEOFILE_METADATA(build_scoped_file_url(@ADVANCED_ANALYTICS.RASTER.FILES, 'WorldClim.zip'), 'WorldClim.shp'));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a9db3fd-b78f-4cc5-a7ec-2a41a154420b",
   "metadata": {
    "name": "markdown_10",
    "collapsed": false,
    "resultHeight": 42
   },
   "source": "It stores spatial objects using the Spatial Reference System `EPSG:4326`. You can examine the Shapefile to check its structure:"
  },
  {
   "cell_type": "code",
   "id": "2e11b147-0e7b-4676-bd07-74b5c28d419c",
   "metadata": {
    "language": "sql",
    "name": "sql_15",
    "collapsed": false,
    "resultHeight": 427
   },
   "outputs": [],
   "source": "SELECT top 10 *\nFROM table(PY_LOAD_GEOFILE(build_scoped_file_url(@ADVANCED_ANALYTICS.RASTER.FILES, 'WorldClim.zip'), 'WorldClim.shp'));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7990bb51-940b-4dd8-9df6-19fd4e4906c9",
   "metadata": {
    "name": "markdown_11",
    "collapsed": false,
    "resultHeight": 93
   },
   "source": "It stores geo objects in the `WKT` column and precipitation (`PREC`) and average temperature (`TAVG`) as properties in a JSON-like object. Knowing this information, it's easy to create a query that reads data from a Shapefile and stores it in a table. This is what you will do in the following query:"
  },
  {
   "cell_type": "code",
   "id": "34f07b7b-e6ac-4335-bca9-4bf4732ededc",
   "metadata": {
    "language": "sql",
    "name": "sql_16",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE ADVANCED_ANALYTICS.RASTER.WORLDWIDE_WEATHER AS\nSELECT to_geography(wkt) as geog, \nproperties:\"PREC\"::float as prec, \nproperties:\"TAVG\"::float as tavg\nFROM table(PY_LOAD_GEOFILE(build_scoped_file_url(@ADVANCED_ANALYTICS.RASTER.FILES, 'WorldClim.zip'), 'WorldClim.shp'));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37df3eae-03c7-4f01-8a72-a67e8178f95a",
   "metadata": {
    "name": "markdown_12",
    "collapsed": false,
    "resultHeight": 328
   },
   "source": "Now that you have a table with average temperature and precipitation, you can visualize it using `pydeck`. Since the newly created table `ADVANCED_ANALYTICS.RASTER.WORLDWIDE_WEATHER` contains more than 800K rows, you may want to reduce its size. In the Python code below, you will do the following:\n- Read data from the weather table, group it using H3 cells of resolution 3, and calculate the average temperature value for each cell.\n- Get the GeoJSON of H3 cell centroids and pass it to a Pandas DataFrame.\n- Extract the longitude and latitude coordinates from the GeoJSON data to prepare it for visualization.\n- Define a color map and assign a color to each data point based on where its value falls within the quantiles.\n- Finally, create a `pydeck` scatterplot layer using the processed data.\nOf course, you could also visualize data using H3 cells as you've done before, but to demonstrate different visualization approaches, you will use points with a radius of 50 km. You can replace `avg(tavg)` with `avg(prec)` to visualize precipitation instead of average temperature."
  },
  {
   "cell_type": "code",
   "id": "81243ce9-8510-4ab4-abc6-ff1ef476345e",
   "metadata": {
    "language": "python",
    "name": "python_2",
    "collapsed": false,
    "resultHeight": 71
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\nimport numpy as np\nimport pydeck as pdk\nimport json\nfrom typing import List\nimport branca.colormap as cm\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\ndf = session.sql('''select st_asgeojson(h3_cell_to_point(h3_point_to_cell(geog, 3))) as geog, \n                    avg(tavg) as value from ADVANCED_ANALYTICS.RASTER.WORLDWIDE_WEATHER\n                    group by all;''').to_pandas()\n\ndf[\"lon\"] = df[\"GEOG\"].apply(lambda row: json.loads(row)[\"coordinates\"][0])\ndf[\"lat\"] = df[\"GEOG\"].apply(lambda row: json.loads(row)[\"coordinates\"][1])\n\n\ndf[\"VALUE\"] = df[\"VALUE\"].apply(lambda row: float(row))\ncenter_latitude = df['lat'].mean()\ncenter_longitude = df['lon'].mean()\n\ndef get_quantiles(df_column: pd.Series, quantiles: List) -> pd.Series:\n    return df_column.quantile(quantiles)\n\ndef get_color(df_column: pd.Series, colors: List, vmin: int, vmax: int, index: pd.Series) -> pd.Series:\n    color_map = cm.LinearColormap(colors, vmin=vmin, vmax=vmax, index=index)\n    return df_column.apply(color_map.rgb_bytes_tuple)\n\nquantiles = get_quantiles(df[\"VALUE\"], [0, 0.2, 0.4, 0.6, 0.8, 1])\ncolors = ['gray','blue','green','yellow','orange','red']\n\ndf['COLOR'] = get_color(df['VALUE'], colors, quantiles.min(), quantiles.max(), quantiles)\n\nst.pydeck_chart(pdk.Deck(\n    map_style=None,\n    initial_view_state=pdk.ViewState(\n        latitude=center_latitude,\n        longitude=center_longitude, pitch=0, zoom=0\n    ),\n    layers=[\n        pdk.Layer(\n            \"ScatterplotLayer\",\n            data=df,\n            get_position=[\"lon\", \"lat\"],\n            opacity=0.9,\n            stroked=True,\n            filled=True,\n            extruded=True,\n            wireframe=True,\n            get_color='COLOR',\n            get_fill_color='COLOR',\n            get_radius=\"50000\",\n            auto_highlight=True,\n            pickable=False,\n        )\n    ],\n))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99c5a13d-eb9c-46cb-a048-79dff5cbf868",
   "metadata": {
    "name": "Step_4_Joining_Data",
    "collapsed": false,
    "resultHeight": 417
   },
   "source": "Now that you have all the data from GeoTiff and Shapefile stored in Snowflake tables, you can join them.\n\n## Step 4. Joining Data from Different Sources\nIn this step, you will join data from two datasets. Let's start by joining the `Elevation` and `Weather` datasets. We observed in the visualizations above that the Elevation dataset covers a relatively small area in Africa, whereas the Weather dataset covers the whole world. To speed up joining these datasets, we can remove from the Weather dataset all points that are outside our area of interest, which corresponds to the coverage area of the Elevation dataset.\n\nIn the query below, you will do the following:\n\n- Use native `ST_` functions to get the minimum and maximum boundaries of the Elevation dataset and create a polygon that corresponds to its outer boundaries.\n- Use the SedonaSnow [ST_Buffer](https://sedona.apache.org/1.5.1/api/snowflake/vector-data/Function/#st_buffer) function to extend that boundary by 0.5 degrees in all directions.\n- Filter out from the Weather dataset all points that are outside the boundaries created in the previous step.\n- Create the `ADVANCED_ANALYTICS.RASTER.WORLDWIDE_WEATHER` table with the new results."
  },
  {
   "cell_type": "code",
   "id": "8859415f-1fdf-4faf-b48f-23d373033b4d",
   "metadata": {
    "language": "sql",
    "name": "sql_18",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE ADVANCED_ANALYTICS.RASTER.AFRICA_WEATHER AS\nwith boundary as (SELECT min(st_xmin(geog)) as xmin, \nmax(st_xmax(geog)) as xmax,\nmin(st_ymin(geog)) as ymin,\nmax(st_ymax(geog)) as ymax,\nsedonasnow.sedona.st_buffer(to_geography('POLYGON ((' || xmin || ' ' || ymin || ', ' ||\n       \t\t\t       xmin || ' ' || ymax || ', ' ||\n       \t\t\t       xmax || ' ' || ymax || ', ' ||\n       \t\t\t       xmax || ' ' || ymin || ', ' ||\n       \t\t\t       xmin || ' ' || ymin ||'))'), 0.5) as external_boundary\nFROM ADVANCED_ANALYTICS.RASTER.AFRICA_ELEVATION)\nselect *\nfrom ADVANCED_ANALYTICS.RASTER.WORLDWIDE_WEATHER,\nboundary\nWHERE ST_INTERSECTS(external_boundary, geog)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1cebde1-c16f-4e0a-913b-d587ab83fbd4",
   "metadata": {
    "name": "markdown_13",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Now check how many points the new dataset contains:"
  },
  {
   "cell_type": "code",
   "id": "7bdc4f8d-c961-4d36-89a0-989d9d8b99f1",
   "metadata": {
    "language": "sql",
    "name": "sql_19",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "select count(*) from ADVANCED_ANALYTICS.RASTER.AFRICA_WEATHER",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f2ae9cfa-0703-43c8-b2fa-1de109e6d60f",
   "metadata": {
    "name": "markdown_14",
    "collapsed": false,
    "resultHeight": 145
   },
   "source": "As a next step, you need to join the `AFRICA_ELEVATION` table with `AFRICA_WEATHER`. Our goal is to find, for each point in the elevation dataset, the closest point from the weather dataset to get weather information. We can do this using different approaches. One approach would be to calculate nearest neighbours using an `ST_DWITHIN`-based join. In the query below, you join two datasets using points that are within 200 km of each other, then partition by objects in `AFRICA_ELEVATION` and, in each partition, sort by distance and keep only the first element:"
  },
  {
   "cell_type": "code",
   "id": "b1c649de-4ba7-4b78-b40f-ba261c05f61a",
   "metadata": {
    "language": "sql",
    "name": "sql_21",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "\nCREATE OR REPLACE TABLE ADVANCED_ANALYTICS.RASTER.ELEVATION_WEATHER_NN AS\nSELECT t1.geog,\n       t1.band,\n       t2.prec,\n       t2.tavg\nFROM ADVANCED_ANALYTICS.RASTER.AFRICA_ELEVATION t1,\n     ADVANCED_ANALYTICS.RASTER.AFRICA_WEATHER t2,\nWHERE ST_DWITHIN(t1.geog, t2.geog, 200000) QUALIFY ROW_NUMBER() OVER \n(PARTITION BY st_aswkb(t1.geog) ORDER BY ST_DISTANCE(t1.geog, t2.geog)) <= 1;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61b970c4-b2e2-4992-bc7a-0917cd044901",
   "metadata": {
    "name": "markdown_15",
    "collapsed": false,
    "resultHeight": 417
   },
   "source": "It took more than 5 minutes on a `LARGE` warehouse. The problem with the query above is that, as a result of the join, it creates an internal table with 1.8 billion rows (12,967,201 × 140), which makes it quite a complex join.\n\nLet's try another approach, which will include two steps:\n\n- For the `AFRICA_WEATHER` table, create a table with Voronoi polygons.\n- Join `AFRICA_ELEVATION` and `AFRICA_WEATHER` using Voronoi polygons and `ST_WITHIN` instead of `ST_DWITHIN`.\n\nA Voronoi polygon is essentially a region consisting of all points closer to a specific seed point than to any other seed point, effectively partitioning space into cells around each seed. So when you join `AFRICA_ELEVATION` with `AFRICA_WEATHER` using points from the elevation table and Voronoi polygons from the weather table, you can be sure that for each elevation point, you are associating it with its nearest weather data.\n\nTo build Voronoi polygons, you will use the [ST_VORONOIPOLYGONS](https://sedona.apache.org/1.5.1/api/snowflake/vector-data/Function/#st_voronoipolygons) function from the [SedonaSnow](https://app.snowflake.com/marketplace/listing/GZTYZF0RTY3/wherobots-ai-sedonasnow) native app. It takes a multi-object as input—in our case, a Multipoint—and returns Voronoi polygons for that object. Since it returns all polygons also as one object, we need a function that converts a multipolygon into multiple separate polygons. Run the following query to create such a UDF:"
  },
  {
   "cell_type": "code",
   "id": "ad1ca758-3ea5-462b-94b5-417afd8d4b85",
   "metadata": {
    "language": "sql",
    "name": "sql_22",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION ADVANCED_ANALYTICS.RASTER.ST_GETPOLYGONS(G OBJECT)\nRETURNS TABLE (POLYGON OBJECT)\nLANGUAGE JAVASCRIPT\nAS '\n{\nprocessRow: function split_multipolygon(row, rowWriter, context){\n    let geojson = row.G;\n    let polygons = [];\n    \n    function extractPolygons(geometry) {\n        if (geometry.type === \"Polygon\") {\n            polygons.push(geometry.coordinates);\n        } else if (geometry.type === \"MultiPolygon\") {\n            for (let i = 0; i < geometry.coordinates.length; i++) {\n                polygons.push(geometry.coordinates[i]);\n            }\n        } else if (geometry.type === \"GeometryCollection\") {\n            for (let i = 0; i < geometry.geometries.length; i++) {\n                extractPolygons(geometry.geometries[i]);\n            }\n        }\n        // Ignore other geometry types (e.g., Point, LineString)\n    }\n    \n    extractPolygons(geojson);\n    \n    for (let i = 0; i < polygons.length; i++) {\n        rowWriter.writeRow({POLYGON: {\n                \"type\" : \"Polygon\",\n                \"coordinates\": polygons[i]\n            }\n        });\n    }\n}\n}\n';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c3d058cf-9013-4232-8068-b1446d2c9462",
   "metadata": {
    "name": "markdown_16",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "In the next query, you build Voronoi polygons for the weather table and enrich the `AFRICA_WEATHER` table with those polygons:"
  },
  {
   "cell_type": "code",
   "id": "02b7900b-f84e-4deb-b04b-ee84df5f202f",
   "metadata": {
    "language": "sql",
    "name": "sql_23",
    "collapsed": false,
    "resultHeight": 87
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE ADVANCED_ANALYTICS.RASTER.AFRICA_WEATHER AS\n-- voronoi_grid CTE that stores all points from AFRICA_WEATHER table \n-- into one object and creates Voronoi Polygons\nwith voronoi_grid as (SELECT sedonasnow.sedona.ST_VoronoiPolygons(st_union_agg(geog)) as polygons\nfrom ADVANCED_ANALYTICS.RASTER.AFRICA_WEATHER),\n-- CTE that flattens results of voronoi_grid CTE\nvoronoi_grid_flattened as (select to_geography(polygon) as polygon\nfrom voronoi_grid,\ntable(ADVANCED_ANALYTICS.RASTER.ST_GETPOLYGONS(st_asgeojson(polygons))))\n-- Below you join table with voronoi polygons and table with weather information\nSELECT *\nFROM ADVANCED_ANALYTICS.RASTER.AFRICA_WEATHER\nINNER JOIN voronoi_grid_flattened\nON ST_WITHIN(geog, polygon);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "83d007e6-40cb-4702-ace2-35ee3af0695c",
   "metadata": {
    "name": "markdown_17",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Now when you have voronoi polygons in `AFRICA_WEATHER` table, you can join `AFRICA_ELEVATION` and `AFRICA_WEATHER`. Run the following query:"
  },
  {
   "cell_type": "code",
   "id": "8349e83e-f92a-4bef-b72c-a28b19a26b47",
   "metadata": {
    "language": "sql",
    "name": "sql_24",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE ADVANCED_ANALYTICS.RASTER.ELEVATION_WEATHER_VORONOI AS\nselect t1.geog, band, prec, tavg\nfrom ADVANCED_ANALYTICS.RASTER.AFRICA_ELEVATION t1\nINNER JOIN ADVANCED_ANALYTICS.RASTER.AFRICA_WEATHER t2\nON ST_INTERSECTS(t1.geog, t2.polygon)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "63d04eb0-415f-434f-998b-e9c37f2f1a4a",
   "metadata": {
    "name": "markdown_18",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Let's look inside of the newly created table:"
  },
  {
   "cell_type": "code",
   "id": "a09a800e-3ffc-444f-8500-8c2b214464ef",
   "metadata": {
    "language": "sql",
    "name": "sql_25",
    "collapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "SELECT TOP 5 * FROM ADVANCED_ANALYTICS.RASTER.ELEVATION_WEATHER_VORONOI",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1535b4b2-da6d-400d-8ed1-8de51f9beb64",
   "metadata": {
    "name": "Advanced_Raster_Use_Case",
    "collapsed": false,
    "resultHeight": 272
   },
   "source": "Now you have data from two unstructured sources stored in a single table. You can use this table to feed into an ML model or enrich it further with some additional features.\n\n## Advanced Raster Use Case\nSometimes raster files can be really large, but as we mentioned earlier, often most of the points contain no data or some default values. As an example, let's look at the raster file from [Forrest Data Lab](https://skogsdatalabbet.se/services/) (`Skogsdatalabbets filserver vid SLU` › `SLU_Forest_Map` › `Tradslag`). `Bok_andel.tif` is 149 MB in size and has a resolution of 52,600×123,200, which results in about 6.5 billion points. Loading all those points would be quite an expensive step, but let's check how band values are distributed inside of that file. Run the following query:"
  },
  {
   "cell_type": "code",
   "id": "8a258d4a-7f39-4f52-92a7-fac4dc4dbe57",
   "metadata": {
    "language": "sql",
    "name": "sql_26",
    "collapsed": false,
    "resultHeight": 439
   },
   "outputs": [],
   "source": "SELECT *\nFROM table(ADVANCED_ANALYTICS.RASTER.PY_RASTER_BAND_VALUE_STATS(build_scoped_file_url(@ADVANCED_ANALYTICS.RASTER.FILES, 'Bok_andel.tif')));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc1f4187-802a-4730-b9fb-69db050a3e69",
   "metadata": {
    "name": "markdown_19",
    "collapsed": false,
    "resultHeight": 118
   },
   "source": "You see that the most frequent band value is 0, which corresponds to 99% of the points. If we load data without those points, we probably won't lose any useful information, but we can have a good saving on compute. Additionally, you can resample the raster to reduce its size. Create a UDF that does both - it resamples to reduce the initial file to the given number of points (50M by default) and ignores given band values:"
  },
  {
   "cell_type": "code",
   "id": "018b4816-b299-4e49-9d79-1be969b9bf2e",
   "metadata": {
    "language": "sql",
    "name": "cell56",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION ADVANCED_ANALYTICS.RASTER.PY_LOAD_GEOTIFF_RESAMPLE_SKIP(\n    PATH_TO_FILE STRING,\n    SKIP_VALUES ARRAY DEFAULT NULL,   -- Optional SKIP_VALUES parameter\n    MAX_PIXELS INT DEFAULT 50000000   -- New optional MAX_PIXELS parameter with default value\n)\nRETURNS TABLE (\n    x FLOAT,\n    y FLOAT,\n    band_values ARRAY,\n    band_count INT\n)\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nPACKAGES = ('numpy', 'rasterio', 'snowflake-snowpark-python')\nHANDLER = 'GeoTiffExtractor'\nAS $$\nimport numpy as np\nimport rasterio\nfrom rasterio.enums import Resampling\nfrom snowflake.snowpark.files import SnowflakeFile\nimport math\n\nclass GeoTiffExtractor:\n    def process(self, PATH_TO_FILE: str, SKIP_VALUES=None, MAX_PIXELS=500000000):\n        try:\n            # Read the GeoTIFF file from the specified stage path\n            with SnowflakeFile.open(PATH_TO_FILE, 'rb', require_scoped_url=False) as input_file:\n                tif_bytes = input_file.read()  # Read the entire file into bytes\n\n            # Use rasterio's MemoryFile to read the TIFF data from memory\n            with rasterio.MemoryFile(tif_bytes) as memfile:\n                with memfile.open() as dataset:\n                    # Get the original dimensions\n                    height = dataset.height\n                    width = dataset.width\n\n                    total_pixels = height * width\n\n                    if total_pixels > MAX_PIXELS:\n                        # Calculate scaling factor\n                        scaling_factor = math.sqrt(MAX_PIXELS / total_pixels)\n                        new_height = int(height * scaling_factor)\n                        new_width = int(width * scaling_factor)\n\n                        # Read the data with the new dimensions\n                        data = dataset.read(\n                            out_shape=(\n                                dataset.count,\n                                new_height,\n                                new_width\n                            ),\n                            resampling=Resampling.average\n                        )\n\n                        # Update the transform for the new dimensions\n                        transform = dataset.transform * dataset.transform.scale(\n                            (width / new_width),\n                            (height / new_height)\n                        )\n                    else:\n                        # Read all bands into a NumPy array\n                        data = dataset.read()  # Shape: (band_count, rows, cols)\n                        transform = dataset.transform\n                        new_height = height\n                        new_width = width\n\n                    # Get the number of bands\n                    band_count = data.shape[0]\n\n                    # Get the coordinates\n                    rows, cols = np.indices((new_height, new_width))\n                    xs, ys = rasterio.transform.xy(\n                        transform, rows, cols, offset='center'\n                    )\n\n                    # Flatten the arrays\n                    xs = np.array(xs).flatten()\n                    ys = np.array(ys).flatten()\n                    pixel_values = data.reshape((band_count, -1)).T  # Shape: (num_pixels, band_count)\n\n                    # Handle SKIP_VALUES\n                    if SKIP_VALUES:\n                        # Convert SKIP_VALUES to a NumPy array for efficient comparison\n                        skip_values = np.array(SKIP_VALUES)\n\n                        # Create a mask for pixels to skip\n                        skip_mask = np.isin(pixel_values, skip_values).any(axis=1)\n                        # Invert the skip_mask to get the mask of pixels to keep\n                        mask = ~skip_mask\n\n                        # Apply the mask to xs, ys, and pixel_values\n                        xs_filtered = xs[mask]\n                        ys_filtered = ys[mask]\n                        pixel_values_filtered = pixel_values[mask]\n                    else:\n                        # If SKIP_VALUES not provided, use all data\n                        xs_filtered = xs\n                        ys_filtered = ys\n                        pixel_values_filtered = pixel_values\n\n                    # For each pixel, yield a row with x, y, and band values\n                    for i in range(len(xs_filtered)):\n                        # Get the pixel values for all bands\n                        band_vals = pixel_values_filtered[i].tolist()\n                        yield (\n                            xs_filtered[i],\n                            ys_filtered[i],\n                            band_vals,\n                            band_count\n                        )\n        except Exception as e:\n            raise Exception(f\"Error during data extraction: {e}\")\n$$;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cd70b92-a1b3-41cb-b1ef-71a18c94daae",
   "metadata": {
    "name": "markdown_20",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Now you can load data from `Bok_andel.tif`. Run the query below to reduce the size of the initial file to 500 million points and ignore points where the band value equals zero."
  },
  {
   "cell_type": "code",
   "id": "c846de9e-fc15-425e-b5f9-350965751945",
   "metadata": {
    "language": "sql",
    "name": "sql_27",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE ADVANCED_ANALYTICS.RASTER.BOK_ANDEL AS\nSELECT x, y,\nband_values[0]::float as band\nFROM table(ADVANCED_ANALYTICS.RASTER.PY_LOAD_GEOTIFF_RESAMPLE_SKIP(build_scoped_file_url(@ADVANCED_ANALYTICS.RASTER.FILES, 'Bok_andel.tif'), [0], 500000000));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d23a4ea0-5e28-46b7-abda-02a8fa7a3615",
   "metadata": {
    "name": "markdown_21",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "In the prevous query you stored `x` and `y` as raw coordinates and the size of the newly created table has 7,028,074 rows. In the following query you check the metadata of the initial file to see what SRID it uses:"
  },
  {
   "cell_type": "code",
   "id": "c64aa1b9-3aa5-45db-a5f1-67ccb39b1c5f",
   "metadata": {
    "language": "sql",
    "name": "sql_28",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "SELECT *\nFROM table(ADVANCED_ANALYTICS.RASTER.PY_EXTRACT_GEOTIFF_METADATA(build_scoped_file_url(@ADVANCED_ANALYTICS.RASTER.FILES, 'Bok_andel.tif')));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0b6b1fb4-a2be-4b8f-9d47-45730079540d",
   "metadata": {
    "name": "markdown_22",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "The SRID is `EPSG:25833`. To store data as `GEOGRAPHY` type for further visualisation you need to convert it into `EPSG:4326`. Run the following query:"
  },
  {
   "cell_type": "code",
   "id": "03ab64b4-1bb5-4a38-8d2e-f1f1767567b6",
   "metadata": {
    "language": "sql",
    "name": "sql_29",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE ADVANCED_ANALYTICS.RASTER.BOK_ANDEL AS\nSELECT TO_GEOGRAPHY(ST_TRANSFORM(ST_MAKEGEOMPOINT(x, y), 25833, 4326)) as geom, band\nFROM ADVANCED_ANALYTICS.RASTER.BOK_ANDEL",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "db80ede4-01e8-4b8a-912c-bd5468fe9ee0",
   "metadata": {
    "name": "markdown_23",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "As a final step you visualize the results using H3 cells:"
  },
  {
   "cell_type": "code",
   "id": "8fe27160-9fb7-402b-8bb5-03341b9e4f3f",
   "metadata": {
    "language": "python",
    "name": "python_3",
    "collapsed": false,
    "resultHeight": 523
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\nimport pydeck as pdk\nfrom typing import List\nimport branca.colormap as cm\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Execute the updated SQL query\ndf = session.sql('''select h3_point_to_cell_string(geom, 7) as h3_cell,\n                    st_x(h3_cell_to_point(h3_cell)) as lon,\n                    st_y(h3_cell_to_point(h3_cell)) as lat,\n                    avg(band) as band \n                    FROM ADVANCED_ANALYTICS.RASTER.BOK_ANDEL\n                    group by all;''').to_pandas()\n\ndf[\"BAND\"] = df[\"BAND\"].apply(lambda row: float(row))\ncenter_latitude = df['LAT'].mean()\ncenter_longitude = df['LON'].mean()\n\ndef get_quantiles(df_column: pd.Series, quantiles: List) -> pd.Series:\n    return df_column.quantile(quantiles)\n\ndef get_color(df_column: pd.Series, colors: List, vmin: int, vmax: int, index: pd.Series) -> pd.Series:\n    color_map = cm.LinearColormap(colors, vmin=vmin, vmax=vmax, index=index)\n    return df_column.apply(color_map.rgb_bytes_tuple)\n    \nquantiles = get_quantiles(df[\"BAND\"], [0, 0.2, 0.4, 0.6, 0.8, 1])\ncolors = ['palegreen', 'lightgreen', 'mediumseagreen', 'forestgreen', 'seagreen', 'darkgreen']\n\ndf['BAND'] = get_color(df['BAND'], colors, quantiles.min(), quantiles.max(), quantiles)\n\nst.pydeck_chart(pdk.Deck(\n    map_style=None,\n    initial_view_state=pdk.ViewState(\n        latitude=center_latitude,\n        longitude=center_longitude, \n        zoom=5.2, \n        bearing=0, \n        pitch=0),\n    layers=[\n        pdk.Layer(\n            \"H3HexagonLayer\",\n            df,\n            opacity=0.9,\n            stroked=False,\n            get_hexagon=\"H3_CELL\",\n            get_fill_color='BAND',\n            extruded=False,\n            wireframe=True,\n            line_width_min_pixels=0,\n            auto_highlight=True,\n            pickable=False,\n            filled=True\n        )\n    ],\n))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d778419-ad2a-4b23-8f7c-4a18c127393f",
   "metadata": {
    "name": "cell1",
    "collapsed": false,
    "resultHeight": 153
   },
   "source": "## Conclusion\n\nIn this lab, you have learned how to load geospatial data from unstructured formats, such as GeoTiff and Shapefiles and what techniques you can apply when you need to join data using nearest neighbout approach. You can use these or similar UDFs to load data from other formats."
  }
 ]
}